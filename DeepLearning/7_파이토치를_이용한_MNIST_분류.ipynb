{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tusker4/Sesac_Saltlux_DeepLearning/blob/main/7_%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_MNIST_%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 덤프"
      ],
      "metadata": {
        "id": "RoZgTbPzSIiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 목적"
      ],
      "metadata": {
        "id": "ZYyg8CPZuYxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- keras 대비 코드량/난이도 체크\n",
        "- 객체 지향 스타일 체크\n",
        "- torch 스타일 체크"
      ],
      "metadata": {
        "id": "NIBeu-kTua2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모듈 가져오기"
      ],
      "metadata": {
        "id": "X1cyZOR0ulWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# 인공 신경망의 레이어들\n",
        "import torch.nn as nn\n",
        "# 망구축용 함수모음\n",
        "import torch.nn.functional as F\n",
        "# 최적화 도구\n",
        "import torch.optim as optim\n",
        "\n",
        "# 데이터 공급자 => 학습에 필요한 데이터 제공\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "SpbSvbBguoOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "DEVICE = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3LUFVw7uz1N",
        "outputId": "0dae0d9d-61e7-45ea-cdf4-bfeb923d5f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 준비\n",
        "\n",
        "- 데이터 공급자를 세팅, 훈련시 공급하게 준비"
      ],
      "metadata": {
        "id": "jDZw5X1Mvf4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 사이즈 (미니 배치 학습, 에포크, 반복회수:이터레이터, 1회학습시 데이터량)\n",
        "BATCH_SIZE = 64 # 설정"
      ],
      "metadata": {
        "id": "FNqEJXluvZ8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터 공급자\n",
        "train_loader = DataLoader(\n",
        "    # 데이터 설정(커스텀, 사전에 준비된 내용)\n",
        "    datasets.FashionMNIST(\n",
        "        root='./data',      # 데이터가 저장된 위치\n",
        "        train=True,         # 훈련용 데이터\n",
        "        download=True,      # 데이터가 없다면 다운로드\n",
        "        # 이미지 전처리\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),    # 데이터를 텐서 형식으로 공급 받겠다\n",
        "            transforms.Normalize( 0.2, 0.3 ) # 평균, 표준편차를 설정해서 정규화 처리 수행\n",
        "        ])\n",
        "    ),                      # 공급할 데이터\n",
        "    batch_size=BATCH_SIZE,  # 배치사이즈\n",
        "    shuffle=True            # 데이터 셔플\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FffYxVJAv2Rg",
        "outputId": "294da774-9e13-4d80-e725-f7986608a322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16973430.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 274046.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 4966107.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 22704812.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 공급자\n",
        "test_loader = DataLoader(\n",
        "    # 데이터 설정(커스텀, 사전에 준비된 내용)\n",
        "    datasets.FashionMNIST(\n",
        "        root='./data',      # 데이터가 저장된 위치\n",
        "        train=False,         # 훈련용 데이터\n",
        "        download=True,      # 데이터가 없다면 다운로드\n",
        "        # 이미지 전처리\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),    # 데이터를 텐서 형식으로 공급 받겠다\n",
        "            transforms.Normalize( 0.2, 0.3 ) # 평균, 표준편차를 설정해서 정규화 처리 수행\n",
        "        ])\n",
        "    ),                      # 공급할 데이터\n",
        "    batch_size=BATCH_SIZE,  # 배치사이즈\n",
        "    shuffle=True            # 데이터 셔플\n",
        ")"
      ],
      "metadata": {
        "id": "UvPNwpDixs19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 동일한 소스에서 훈련, 테스트 데이터를 공급하므로, 중복 데이터 가능성 열어두고 진행"
      ],
      "metadata": {
        "id": "KLIBl7vTyDcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인공신경망 구축 - 객체 지향 스타일"
      ],
      "metadata": {
        "id": "glM2N-IoyX1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "- 입력층\n",
        "- 은닉층\n",
        "    L 합성곱층\n",
        "    L 풀링층\n",
        "    L 합성곱층\n",
        "    L 풀링층\n",
        "    L flattern\n",
        "    L 전결합층\n",
        "- 출력층\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Gn3eGbQzydrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    #name = 'torch cnn'\n",
        "    # 생성자\n",
        "    def __init__(self):\n",
        "        '''\n",
        "            1. 명시적 상속 => 부모 클레스(super)의 생성자 호출 진행\n",
        "            2. 각 신경망의 레이어들 생성(맴버 변수로)\n",
        "        '''\n",
        "        super(Net, self).__init__()\n",
        "        # 맴버 접근은 객체명.맴버\n",
        "        # FashionMNIST -> 28x28 grayscale(1채널)\n",
        "        # 풀링, 플랫툰 등은 신경망 연결시 세팅(만들기 나름)\n",
        "        self.conv1   = nn.Conv2d(1, 32, 5, 1, padding='same')    # 합성곱 1층\n",
        "        self.conv2   = nn.Conv2d(32,32*2, 5, 1, padding='same')  # 합성곱 2층\n",
        "        self.dropout = nn.Dropout2d(0.1) # 과적합 방지\n",
        "        # 28 -> 14 -> 7 -> flattern : 7*7*32*2 (4D->2D)\n",
        "        self.fc      = nn.Linear(7*7*32*2, 1024) # 3136 -> 1024로 수렴\n",
        "        self.output  = nn.Linear(1024, 10)\n",
        "        pass\n",
        "\n",
        "    # 맴버함수, 부모의 함수를 재정의(overide)\n",
        "    # 순전파 신경망 (x->y), 인공신경망 연결\n",
        "    # 역전파 신경망을 재정의 하지 않으면 부모가 세팅한 방식 그대로 사용한다는 의미\n",
        "    # 순전파 방향으로 구성된 인공신경망(시퀀스 형태)를 리턴\n",
        "    def forward(self, x):  # x는 입력층\n",
        "        # 합성곱 1f (합성곱 + 최대풀링(레이어로 않하고 함수로 처리, 설정)포함)\n",
        "        # 이미지 크기(h,w) : 28 -> 14\n",
        "        # 객체를 함수처럼 사용 __call__ 구현되어 있으면 전달 가능\n",
        "        x = F.relu( F.max_pool2d( self.conv1(x), 2 ) ) # 2:커널사이즈, stride는 동일값\n",
        "        # 합성곱 2f conv2에 dropout 추가\n",
        "        # 이미지 크기(h,w) : 14 -> 7\n",
        "        x = F.relu( F.max_pool2d( self.dropout(self.conv2(x)), 2 ) )\n",
        "        # 4D -> 2D : Flattern => torch에서는 view => 데이터의 순서가 바뀌지 않는다\n",
        "        # -1은 뒤를 채우고 나머지 수치는 앞을 채워라.\n",
        "        # 훈련시, 데이터를 가변적으로 넣을수 있다\n",
        "        x = x.view( -1, 7*7*32*2)\n",
        "        # 3131 -> 1024\n",
        "        x = self.dropout(F.relu( self.fc(x) ))\n",
        "        # 1023 -> 10\n",
        "        # softmax 혹은 log_softmax 사용 -> 데이터가 원본이면(softmax)\n",
        "        # 정규화를 통해서 분포가 변경되고, 데이터도 변경됨 -> log_softmax 진행\n",
        "        return F.log_softmax( self.output( x ), dim=1 )  # dim=1 출력값 shape 조정\n",
        "\n",
        "\n",
        "\n",
        "        pass\n",
        "    pass\n",
        "\n",
        "# 인공 신경망 생성후 cpu 혹은 cuda 사용 지정\n",
        "model = Net().to(DEVICE)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnbEkV8ix--I",
        "outputId": "feec0693-bb10-460a-b63b-f33592000721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
              "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "  (fc): Linear(in_features=3136, out_features=1024, bias=True)\n",
              "  (output): Linear(in_features=1024, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습에 필요한 도구 준비\n",
        "\n",
        "- 케라스에서는 모델 컴파일\n",
        "    - 최적화도구(GD, SGD,.)\n",
        "    - 손실함수(크로스엔트로피(이진:로지스틱회귀+시그모이드,다중분류:소프트맥스회귀))\n",
        "    - 평가지표(정확도,...)\n",
        "\n",
        "- 토치\n",
        "    - 학습 및 테스트 진행시 손실함수, 평가지표 사용 -> 학습 함수, 테스트 함수를 만들어서 내부에서 체크"
      ],
      "metadata": {
        "id": "HSugwG4kwHYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적화 도구\n",
        "# model.parameters() : 신경망 구성으로 발생된 w와 b를 접급할수 있는 객체 => 최적화의 대상\n",
        "# lr : 학습률\n",
        "# momentum : 가속도\n",
        "optimzer = optim.SGD( model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "hq-35fnQzH8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 훈련"
      ],
      "metadata": {
        "id": "YrzFxrbF0KGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련용 함수"
      ],
      "metadata": {
        "id": "sixyY88I0L50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train( model, train_loader, optimzer, epoch ):\n",
        "    '''\n",
        "        - model : 인공신경망, 학습할 모델\n",
        "        - train_loader : 훈련 데이터 공급자\n",
        "        - optimzer : 최적화 도구\n",
        "        - epoch : 에폭 정보 (1 ~ 10)\n",
        "    '''\n",
        "    # 1. 모델을 학습 모드로 전환\n",
        "    model.train()\n",
        "    # 2. 미니 배치 학습 진행 -> 전체 데이터를 배치사이즈 대비 나눈값양으로 훈련진행\n",
        "    for idx, (data, target) in enumerate( train_loader ): # 인덱스, 피처데이터, 정답데이터\n",
        "        # 3. cpu, gpu를 데이터에 설정\n",
        "        data   = data.to(DEVICE)\n",
        "        target = target.to(DEVICE)\n",
        "        # 4. 최적화 도구 초기화, 최적화한 내용은 밑에서 기록되고, 새로 학습 진행되면 초기화\n",
        "        #    한번의 학습이 완료되면 (Iteration이 한번 끝나면) gradients가 기록(밑에서)되고\n",
        "        #    다음 학습을 위해서 항상 0으로 만들어 줘야함\n",
        "        optimzer.zero_grad()\n",
        "        # 5. 모델에 데이터를 주입하여 결과를 얻는다\n",
        "        ouptut = model( data ) # 예측\n",
        "        # 6. 실제값과 예측값 사이의 차이 계산 => 손실함수\n",
        "        loss   = F.cross_entropy( ouptut, target )\n",
        "        # 7. 오차역전파 진행 : y -> x 이동 => 가중치 업데이트 => 최적화 수행\n",
        "        #    gradients가 기록한다\n",
        "        loss.backward()\n",
        "        # 8. 최종적으로 만들어진 최적 파라미터(w, b)를  모델에 최종 반영\n",
        "        optimzer.step()\n",
        "        # 9. 로그 출력 : vervose에서 컨트롤\n",
        "        if idx % 200 == 0: # 특정단위가 도래하면 로그 출력\n",
        "            print( f'Epoch:{epoch}\\t batch cnt:{idx}\\t loss={loss.item()}')\n",
        "        pass\n",
        "    # 체크 포인트 - 모델 덤프 (pt or pth)\n",
        "    torch.save({\n",
        "        'model' : 'train-model'\n",
        "    }, f'./model-checkpoint-{epoch}.pt')\n",
        "    pass"
      ],
      "metadata": {
        "id": "KITpMThIxVqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테스트용 함수"
      ],
      "metadata": {
        "id": "0O3vkxn00OL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test( model, test_loader ):\n",
        "    '''\n",
        "        세대별(1epoch, 2epoch,..) 학습 결과를 모아서 평균 손실, 정확도 출력\n",
        "        - model : 인공신경망, 학습할 모델\n",
        "        - test_loader : 테스트 데이터 공급자\n",
        "    '''\n",
        "    # 0. 손실, 정확도를 담을 값을 누적해서 데이터 총개수로 나누면 체크 완료\n",
        "    loss, acc = (0,0)\n",
        "    # 1. 모델을 테스트 모드로 전환\n",
        "    model.eval()\n",
        "    # 2. 테스트가 진행되는 동안 모든 결과는 기록하지 않는다 -> 학습에 영향을 않미친다\n",
        "    with torch.no_grad(): # 계산 X\n",
        "        # 3. 미니 배치 학습 진행 -> 전체 데이터를 배치사이즈 대비 나눈값양으로 훈련진행\n",
        "        for idx, (data, target) in enumerate( train_loader ): # 인덱스, 피처데이터, 정답데이터\n",
        "            # 4. cpu, gpu를 데이터에 설정\n",
        "            data, target   = data.to(DEVICE), target.to(DEVICE)\n",
        "            # 5. 모델에 데이터를 주입하여 결과를 얻는다\n",
        "            ouptut = model( data ) # 예측\n",
        "            # 6. 실제값과 예측값 사이의 차이 계산 => 손실함수\n",
        "            #    64개 데이터를 가져와서 손실계산 => 64개의 손실값이 등장\n",
        "            #    reduction='sum' : 텐서 맴버들을 모두 더해라\n",
        "            #    item() 텐서의 값 추출(맴버가 1개만 가능)\n",
        "            loss   += F.cross_entropy( ouptut, target, reduction='sum').item()\n",
        "            # 7. 정확도 -> 예측값 필요\n",
        "            #    10개로 분류된 결과는 확률 -> 가장 높은 확률 -> 분류값\n",
        "            #    numpy.argmax() : 최고값을 가진 인덱스 번호 리턴\n",
        "            pred = ouptut.max(1, keepdim=True)[1] # 최고값을 가진 인덱스 번호를 shape유지해서 리턴\n",
        "            #    target에 정답 인덱스가 있다\n",
        "            # 8. 정확도 체크\n",
        "            #    target과 pred간 데이터별 예측값이 일치 => 인덱스 값이 일치 체크\n",
        "            #    행렬비교 => True, False,... => 1, 0,.. => sum() => 텐서, 값1개 => item() => 값\n",
        "            #    64개를 예측해서 55개 맞췄다\n",
        "            acc  += pred.eq( target.view_as( pred ) ).sum().item() # 맞춘개수\n",
        "\n",
        "            pass\n",
        "    # 특정 epoch의 평균 손실, 평균 정확도\n",
        "    mean_loss = loss / len( test_loader.dataset )\n",
        "    mean_acc  = acc  / len( test_loader.dataset ) * 100 # 퍼센트\n",
        "    return mean_loss, mean_acc\n",
        "    pass"
      ],
      "metadata": {
        "id": "DtvZ2UE_0QIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 진행"
      ],
      "metadata": {
        "id": "oX10jVqF0W40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10 # 총 10세대 학습 진행\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    # 훈련\n",
        "    train( model, train_loader, optimzer, epoch )\n",
        "    # 테스트\n",
        "    mean_loss, mean_acc = (  model, test_loader )\n",
        "    # 로그출력\n",
        "    print( f'epoch:{epoch}\\t loss={mean_loss}\\t acc={mean_acc}')"
      ],
      "metadata": {
        "id": "0q7lb0K70Z1U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a5d75b-69de-4e13-afa4-ddfe131e67a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1\t batch cnt:0\t loss=2.3151438236236572\n",
            "Epoch:1\t batch cnt:200\t loss=0.7033793926239014\n",
            "Epoch:1\t batch cnt:400\t loss=0.6762149333953857\n",
            "Epoch:1\t batch cnt:600\t loss=0.44463682174682617\n",
            "Epoch:1\t batch cnt:800\t loss=0.3419739305973053\n",
            "epoch:1\t loss=Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=3136, out_features=1024, bias=True)\n",
            "  (output): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\t acc=<torch.utils.data.dataloader.DataLoader object at 0x7f2ec4b2fd60>\n",
            "Epoch:2\t batch cnt:0\t loss=0.385941743850708\n",
            "Epoch:2\t batch cnt:200\t loss=0.33146223425865173\n",
            "Epoch:2\t batch cnt:400\t loss=0.4148309528827667\n",
            "Epoch:2\t batch cnt:600\t loss=0.37486666440963745\n",
            "Epoch:2\t batch cnt:800\t loss=0.2281140238046646\n",
            "epoch:2\t loss=Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=3136, out_features=1024, bias=True)\n",
            "  (output): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\t acc=<torch.utils.data.dataloader.DataLoader object at 0x7f2ec4b2fd60>\n",
            "Epoch:3\t batch cnt:0\t loss=0.28966256976127625\n",
            "Epoch:3\t batch cnt:200\t loss=0.26836156845092773\n",
            "Epoch:3\t batch cnt:400\t loss=0.5053390264511108\n",
            "Epoch:3\t batch cnt:600\t loss=0.40433403849601746\n",
            "Epoch:3\t batch cnt:800\t loss=0.34232133626937866\n",
            "epoch:3\t loss=Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=3136, out_features=1024, bias=True)\n",
            "  (output): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\t acc=<torch.utils.data.dataloader.DataLoader object at 0x7f2ec4b2fd60>\n",
            "Epoch:4\t batch cnt:0\t loss=0.34236133098602295\n",
            "Epoch:4\t batch cnt:200\t loss=0.3166109621524811\n",
            "Epoch:4\t batch cnt:400\t loss=0.26236748695373535\n",
            "Epoch:4\t batch cnt:600\t loss=0.3113459348678589\n",
            "Epoch:4\t batch cnt:800\t loss=0.23277735710144043\n",
            "epoch:4\t loss=Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=3136, out_features=1024, bias=True)\n",
            "  (output): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\t acc=<torch.utils.data.dataloader.DataLoader object at 0x7f2ec4b2fd60>\n",
            "Epoch:5\t batch cnt:0\t loss=0.2666455805301666\n",
            "Epoch:5\t batch cnt:200\t loss=0.27549421787261963\n",
            "Epoch:5\t batch cnt:400\t loss=0.252184122800827\n",
            "Epoch:5\t batch cnt:600\t loss=0.33200934529304504\n",
            "Epoch:5\t batch cnt:800\t loss=0.27044039964675903\n",
            "epoch:5\t loss=Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=3136, out_features=1024, bias=True)\n",
            "  (output): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\t acc=<torch.utils.data.dataloader.DataLoader object at 0x7f2ec4b2fd60>\n",
            "Epoch:6\t batch cnt:0\t loss=0.33051177859306335\n",
            "Epoch:6\t batch cnt:200\t loss=0.13875828683376312\n",
            "Epoch:6\t batch cnt:400\t loss=0.12568265199661255\n",
            "Epoch:6\t batch cnt:600\t loss=0.28610190749168396\n",
            "Epoch:6\t batch cnt:800\t loss=0.1988391876220703\n",
            "epoch:6\t loss=Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=3136, out_features=1024, bias=True)\n",
            "  (output): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\t acc=<torch.utils.data.dataloader.DataLoader object at 0x7f2ec4b2fd60>\n",
            "Epoch:7\t batch cnt:0\t loss=0.22469913959503174\n",
            "Epoch:7\t batch cnt:200\t loss=0.2090812772512436\n",
            "Epoch:7\t batch cnt:400\t loss=0.23098686337471008\n",
            "Epoch:7\t batch cnt:600\t loss=0.350016713142395\n",
            "Epoch:7\t batch cnt:800\t loss=0.4559936225414276\n",
            "epoch:7\t loss=Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=3136, out_features=1024, bias=True)\n",
            "  (output): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\t acc=<torch.utils.data.dataloader.DataLoader object at 0x7f2ec4b2fd60>\n",
            "Epoch:8\t batch cnt:0\t loss=0.27743443846702576\n",
            "Epoch:8\t batch cnt:200\t loss=0.16493529081344604\n",
            "Epoch:8\t batch cnt:400\t loss=0.24314138293266296\n",
            "Epoch:8\t batch cnt:600\t loss=0.09720417857170105\n",
            "Epoch:8\t batch cnt:800\t loss=0.18182958662509918\n",
            "epoch:8\t loss=Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=3136, out_features=1024, bias=True)\n",
            "  (output): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\t acc=<torch.utils.data.dataloader.DataLoader object at 0x7f2ec4b2fd60>\n",
            "Epoch:9\t batch cnt:0\t loss=0.2410474717617035\n",
            "Epoch:9\t batch cnt:200\t loss=0.10639015585184097\n",
            "Epoch:9\t batch cnt:400\t loss=0.10545488446950912\n",
            "Epoch:9\t batch cnt:600\t loss=0.263071745634079\n",
            "Epoch:9\t batch cnt:800\t loss=0.23139236867427826\n",
            "epoch:9\t loss=Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=3136, out_features=1024, bias=True)\n",
            "  (output): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\t acc=<torch.utils.data.dataloader.DataLoader object at 0x7f2ec4b2fd60>\n",
            "Epoch:10\t batch cnt:0\t loss=0.25720730423927307\n",
            "Epoch:10\t batch cnt:200\t loss=0.2179175168275833\n",
            "Epoch:10\t batch cnt:400\t loss=0.37880489230155945\n",
            "Epoch:10\t batch cnt:600\t loss=0.2308478206396103\n",
            "Epoch:10\t batch cnt:800\t loss=0.09922599047422409\n",
            "epoch:10\t loss=Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=3136, out_features=1024, bias=True)\n",
            "  (output): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\t acc=<torch.utils.data.dataloader.DataLoader object at 0x7f2ec4b2fd60>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WK52ClPrRwBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 개별 모듈 테스트"
      ],
      "metadata": {
        "id": "cNUqb2GK5ug_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor( [1,1,2,0] )\n",
        "b = torch.Tensor( [0,0,1,1] )\n",
        "\n",
        "F.cross_entropy( a, b, reduction='sum',  ).item()"
      ],
      "metadata": {
        "id": "bzAWH22V5t8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aadb50d-1dae-411f-c784-f2023a4af95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.253046989440918"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor( [1,1,2,0] )\n",
        "b = torch.Tensor( [[1,0,1,1]] )\n",
        "a.size(), b.size()"
      ],
      "metadata": {
        "id": "vJDliW8FPfZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56fdb7a-d2c9-4424-f750-15207a99e9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4]), torch.Size([1, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 특정 차원의 텐서를 특정 차원의 텐서 형태로 변환, 비교, 합산, 값추출\n",
        "a.view_as(b), b.eq(a.view_as(b)), b.eq(a.view_as(b)).sum(), b.eq(a.view_as(b)).sum().item()"
      ],
      "metadata": {
        "id": "th9lXLovPni-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da24edca-78eb-4826-d3db-173468486c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1., 1., 2., 0.]]),\n",
              " tensor([[ True, False, False, False]]),\n",
              " tensor(1),\n",
              " 1)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.arange(16).view(4,4)\n",
        "print( tensor, tensor.size() )\n",
        "# 4개의 데이터에서 각각 최고값을 가진 값 혹은 인덱스 추출\n",
        "print( '-'*10)\n",
        "print( tensor.max() ) # 전체 구성원중 가장 높은값 추출\n",
        "print( '-'*10)\n",
        "print( tensor.max(1) ) # 1 => dim 값이 두번째 => 2차원\n",
        "print( '-'*10)\n",
        "print( tensor.max(1)[1] ) # 각 데이터에서 인덱스 번호 3번이 최대값의 자리이다\n",
        "print( '-'*10)\n",
        "print( tensor.max(1, keepdim=True)[1] ) # 차원유지 => 2차원유지, 최대값을 가진 인덱스만 추출"
      ],
      "metadata": {
        "id": "ov-5c0Nc01BE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b246282d-01b1-44e4-9dd1-d53301e691c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11],\n",
            "        [12, 13, 14, 15]]) torch.Size([4, 4])\n",
            "----------\n",
            "tensor(15)\n",
            "----------\n",
            "torch.return_types.max(\n",
            "values=tensor([ 3,  7, 11, 15]),\n",
            "indices=tensor([3, 3, 3, 3]))\n",
            "----------\n",
            "tensor([3, 3, 3, 3])\n",
            "----------\n",
            "tensor([[3],\n",
            "        [3],\n",
            "        [3],\n",
            "        [3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9xhQLlen8vPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 덤프"
      ],
      "metadata": {
        "id": "uxeYxd7MSC2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 전체 저장, 불러오기"
      ],
      "metadata": {
        "id": "hNFHbakASFGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 전체 저장, 불러오기"
      ],
      "metadata": {
        "id": "4P0wj4KlSHYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 덤프"
      ],
      "metadata": {
        "id": "wZWIZUcsSJvv"
      }
    }
  ]
}
